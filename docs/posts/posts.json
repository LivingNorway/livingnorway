[
  {
    "path": "posts/Butterflies/",
    "title": "Exporting complex ecological data into GBIF",
    "description": "To GBIF and back again, using the Event core standard.",
    "author": [
      {
        "name": "Jens Åström",
        "url": "https://github.com/jenast"
      }
    ],
    "date": "2020-08-11",
    "categories": [],
    "contents": "\r\nWhere should I put my ecological data set?\r\nMaking your data publicly available is quickly becoming a standard task for researchers. It is increasingly demanded by journals when publishing your research findings, or even by funding agencies when applying for grants. Journals have traditionally accepted data in file format, which can be reached through their websites along with the paper. But these data have idiosyncratic formats and are typically stripped down to a minimal set suited for a specific analysis in the paper. In addition, the data can be hard to find and cumbersome to download, since it is stored (or should I say hidden) behind paywalls on various publishers’ websites.\r\nWouldn’t it be nice if we could store our ecological data using a common format, in a common place, freely available for everyone?\r\nGBIF could be your solution. It doesn’t work for every sort of biological data, but it probably works for more cases than you would think.\r\n-But my data have a complex structure with repeated measurements and zero-counts that GFIB just can’t accept, you might say.\r\nWell let’s see about that. With the new(ish) “Event Core” of the Darwin core standard, odds are that GBIF works for your data as well. Below is the story of how I formatted and published a multi-year observation data set of ~80 species with a hierarchical survey scheme, while incorporating all collected environmental covariates, and meta-data into GBIF.\r\nThe Event Core standard\r\nGBIF — the Global Biodiversity Information Facility, is a vast store of biodiversity data, with records spanning hundreds of years, and containing hundreds of millions of occcurrence records. Up until recently though, it has been limited to presence-only data, neglecting information of possible zero-counts. The addition of Events to the Darwin Core standard seeks to remedy that. With this addition, GBIF now accepts most survey data. It does this by organizing the data around individual sample events, i.e. a distinct measurement event of one or more species together with related records, in a specific time and place.\r\nSee here for info on the Darwin core in general: https://www.gbif.org/darwin-core. For event-data specifically, see here: https://www.gbif.org/data-quality-requirements-sampling-events.\r\nFrom a custom sample structure to a standardized hierarchy of events\r\nMost of our complex data has a structure to it, whether it is spelled out clearly through naming schemes, or implicit through relationships within a database. A botanist for example, might have mapped the flora of a forest stand by placing 1\\(m^2\\) survey squares randomly 10 times, and recorded the percentage cover of each species in the squares. The individual observations of the plants are then linked together through the specific 1\\(m^2\\) survey square they were found in, and further linked together through the 10 survey squares in the same forest stand. This could be recorded in a single table by using a column for the names of the forest stand, a column for the survey square a column for the plant species and a column of the percentage cover of the species (Table 1). Or, the data could be split up into separate tables in a database, with keys that link the tables together. Alas, there are (almost) as many potential structures as there are surveys. But there is only one Darwin Event Core. The task is then to standardise individual data sets so that it fits into GBIF, while providing the necessary information so that the original format can later be retrieved. It’s a little bit like zipping and unzipping a file.\r\nThe key idea behind the event core format is to link related information in a data set together using a hierarchy of recording events. The lowest level event in our example is the counting of plants within one survey square. That’s the smallest observational unit that links individual records together. Each such event has a number of occurrence records linked to it, in this example the percentage cover of each plant in that survey square. This can include values of 0%, for species that we looked for, but didn’t find in any particular survey square. We can also include zero counts the same way. The hierarchical structure of the survey scheme is recorded by encoding the higher recording level, above the observation event as a parent event. In our case this is the survey of the entire forest stand. This hierarchy is open-ended, meaning there could be additional higher level events. Thus a parent event can have it’s own parent event, and so on.\r\nThe point of coding the hierarchical relationships this way is to be able to store any (?) structure and related recorded data in a standardised way, in only one event table and one occurrence table. This obviously has some major benefits. We could store a wide range of biodiversity data in a unified way at a single location, facilitating the combination of data sources and streamlining data gathering routines.\r\nBut nothing so great comes for free, and the cost here is that you might need to rearrange your data before you upload it to GBIF. This particular formatting can seem confusing at first, and along the way you might wonder if it’s really worth it. But there is no way around it. There is inherently work involved in organizing complex data into a unified format and getting it back again. But once it’s done, your data is securely stored and publicly available, free of charge, in the worlds largest repository of biodiversity information.\r\nA simple example\r\nLet’s look at our made up example data of plant surveys in forests.\r\n\r\n\r\n\r\n\r\nTable 1: Made up simple example data.\r\ndate\r\nobserver\r\nforest\r\nplot\r\npH\r\nspecies\r\npercCover\r\n2019-04-01\r\nMary\r\nSherwood\r\nplot_1\r\n6.23\r\nQuercus robur\r\n34.44\r\n2019-04-01\r\nMary\r\nSherwood\r\nplot_1\r\n6.23\r\nAnemone nemorosa\r\n25.61\r\n2019-04-01\r\nMary\r\nSherwood\r\nplot_1\r\n6.23\r\nAthyrium filix-femina\r\n0.38\r\n2019-04-01\r\nMary\r\nSherwood\r\nplot_2\r\n7.24\r\nQuercus robur\r\n9.30\r\n2019-04-01\r\nMary\r\nSherwood\r\nplot_2\r\n7.24\r\nAnemone nemorosa\r\n26.64\r\n2019-04-01\r\nMary\r\nSherwood\r\nplot_2\r\n7.24\r\nAthyrium filix-femina\r\n20.57\r\n2019-04-15\r\nJohn\r\nNottingham\r\nplot_1\r\n7.22\r\nQuercus robur\r\n27.74\r\n2019-04-15\r\nJohn\r\nNottingham\r\nplot_1\r\n7.22\r\nAnemone nemorosa\r\n21.80\r\n2019-04-15\r\nJohn\r\nNottingham\r\nplot_1\r\n7.22\r\nAthyrium filix-femina\r\n11.31\r\n2019-04-15\r\nJohn\r\nNottingham\r\nplot_2\r\n7.25\r\nQuercus robur\r\n36.94\r\n2019-04-15\r\nJohn\r\nNottingham\r\nplot_2\r\n7.25\r\nAnemone nemorosa\r\n11.69\r\n2019-04-15\r\nJohn\r\nNottingham\r\nplot_2\r\n7.25\r\nAthyrium filix-femina\r\n33.49\r\n\r\n\r\n\r\n\r\nFigure 1: Rare footage of early botanists.\r\n\r\n\r\n\r\nUsing the Event Core format, this could be transformed into something like these two tables, which GBIF can swallow.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nTable 2: Minimal example of Event table for made up data.\r\neventDate\r\neventID\r\nparentEventID\r\nlocationID\r\ndynamicProperties\r\n2019-04-01\r\nparentEvent_1\r\nNA\r\nSherwood\r\n{“Mary”}\r\n2019-04-01\r\nevent_1\r\nparentEvent_1\r\nplot_1\r\n{“pH” : 6.23}\r\n2019-04-01\r\nevent_2\r\nparentEvent_1\r\nplot_2\r\n{“pH” : 7.24}\r\n2019-04-15\r\nparentEvent_2\r\nNA\r\nNottingham\r\n{“John”}\r\n2019-04-15\r\nevent_3\r\nparentEvent_2\r\nplot_1\r\n{“pH” : 7.22}\r\n2019-04-15\r\nevent_4\r\nparentEvent_2\r\nplot_2\r\n{“pH” : 7.25}\r\n\r\n\r\nTable 3: Minimal example of Occurrence table for made up data.\r\neventID\r\nscientificName\r\norganismQuantity\r\norganismQuantityType\r\nevent_1\r\nQuercus robur\r\n34.44\r\npercentageCover\r\nevent_1\r\nAnemone nemorosa\r\n25.61\r\npercentageCover\r\nevent_1\r\nAthyrium filix-femina\r\n0.38\r\npercentageCover\r\nevent_2\r\nQuercus robur\r\n9.30\r\npercentageCover\r\nevent_2\r\nAnemone nemorosa\r\n26.64\r\npercentageCover\r\nevent_2\r\nAthyrium filix-femina\r\n20.57\r\npercentageCover\r\nevent_3\r\nQuercus robur\r\n27.74\r\npercentageCover\r\nevent_3\r\nAnemone nemorosa\r\n21.80\r\npercentageCover\r\nevent_3\r\nAthyrium filix-femina\r\n11.31\r\npercentageCover\r\nevent_4\r\nQuercus robur\r\n36.94\r\npercentageCover\r\nevent_4\r\nAnemone nemorosa\r\n11.69\r\npercentageCover\r\nevent_4\r\nAthyrium filix-femina\r\n33.49\r\npercentageCover\r\n\r\nNotice that the individual survey squares are tied to a common forest survey through the parentEventID column in the event table. Additional environmental data (here pH and observer) is stored in a column named “dynamicProperties”, which is formated as JSON, and can hold any collection of such extra information. The result is perhaps less readable for a human, but the structure is generalizable to a lot of situations.\r\nButterfly and bumblebee example\r\nI will now show how we formatted a real world data set for publication on GBIF through the Event Core standard. The data comes from a yearly survey of butterfly and bumblebees at approximately 60 fixed locations in Norway. We employ citizen scientists to record the abundances of butterflies and bumblebees along 20 50 meter long pre-defined transects at each survey location. Each location is visited 3 times each year, and rudimentary environmental data is recorded at each transect, each visit. We train each surveyor to be able to identify all occurring species, and thus record non observed species as zero counts.\r\n\r\n\r\n\r\nFigure 2: One of the survey squares with its 20 transects á 50m\r\n\r\n\r\n\r\nHere, each transect walk along the individual 50 meter transects form the lowest level observation event. Within each of these transect walks, each species can be observed 0 or more times. The 20 transects are clustered together within a larger survey square of 1.5x1.5 kilometers and together characterize the bumblebee and butterfly communities at these locations. We therefore can format the data into one table of occurrences, and one table of events, including the transect walks as the lowest level events and the survey square visits as parentEvents.\r\nIn the toy example above, we used readable id’s for the sampling units (Sherwood, plot_1, etc.) for pedagogical reasons, but it is good practice to use unique identifiers whenever possible in your real data. Thus in our example, every event, parent event, and location is replaced with a UUID, which is stable throughout time. In our case, we might relocate transect nr 5 in a survey location due to practical reasons, but keep the name of “transect 5”. But this transect then gets a new unique identifier, making it possible to distinguish the new and old locations.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nThe event table\r\nThis is how two rows of our entire event table looks like as a tibble, showing one parent event (survey square visit) and one of it’s children events (transect walks).\r\n\r\n\r\n# A tibble: 2 x 26\r\n  id    type  modified            ownerInstitutio… dynamicProperti…\r\n  <chr> <chr> <dttm>              <chr>            <chr>           \r\n1 fffc… Event 2020-04-20 14:10:32 Miljødirektorat…  <NA>           \r\n2 6aab… Event 2020-04-20 14:10:32 Miljødirektorat… \"{\\\"observerID\\…\r\n# … with 21 more variables: eventID <chr>, parentEventID <chr>,\r\n#   samplingProtocol <chr>, sampleSizeValue <dbl>,\r\n#   sampleSizeUnit <chr>, eventDate <dttm>, eventTime <chr>,\r\n#   eventRemarks <chr>, locationID <chr>, country <chr>,\r\n#   countryCode <chr>, stateProvince <chr>, municipality <chr>,\r\n#   locality <chr>, locationRemarks <chr>, decimalLatitude <dbl>,\r\n#   decimalLongitude <dbl>, geodeticDatum <chr>,\r\n#   coordinateUncertaintyInMeters <dbl>, footprintWKT <chr>,\r\n#   footprintSRS <chr>\r\n\r\nThe first 5 columns records the record id, the type of record, the latest time the data was modified, the owner of the data and additional “dynamic properties”. Like in the toy example above, the dynamic properties contain various data that is tied to the observation event, that doesn’t have their own predefined GBIF columns. For our data set, this includes the person who recorded the observation, the local habitat type, cloud cover, temperature, and the local flower cover. This is concatenated into a JSON-string, in order for the GBIF database to be able to accept various formats. In our data, we don’t have any dynamic properties (extra information that GBIF lacks columns for) that are tied to the parent event (larger survey square level). It is for example possible to have two observers sharing a survey square by dividing up the transects withing it (although uncommon). I imagine one could replicate any dynamic properties of the parent event down to the child events if one would like, but that would result in unecessary data size, and less clarity of the hierarchical nature of the data. This is what our whole string of dynamic properties looks like:\r\n\r\ndynamicProperties\r\n{“observerID” : “98021957-065d-4ea4-9453-9e91fc1c8b61”, “habitatType” : “forest”, “cloudCover%” : 5, “temperatureCelsius” : 23, “flowerCover0123” : 1}\r\n\r\n\r\n\r\n\r\nFigure 3: Pollinator survey along a transect. Photo: Sondre Dahle.\r\n\r\n\r\n\r\nNext we have the eventIDs, parentEventIDs, sampling protocol, sample size and sample size unit. Note that the eventID for the parent event (the survey of the whole 1.5x1.5km square) is recorded in the parentID column for the respective “child-events”, i.e. the individual transect walks. Notice also that the sampleSizeValue records the full sample size at that level, where twenty 50 meter long transects combine to make one 1000 m long parent event.\r\n\r\neventID\r\nparentEventID\r\nsampleSizeValue\r\nsampleSizeUnit\r\nfffcc609-fbba-42c1-b264-0912d9d7b2af\r\nNA\r\n1000\r\nmetre\r\n6aab1176-572b-4e13-b7d1-6d2a5a217961\r\nfffcc609-fbba-42c1-b264-0912d9d7b2af\r\n50\r\nmetre\r\n\r\nThe next colums is eventDate, eventTime, and a remark for the observation level, where information about the hierarchical levels can be noted. Note that the eventTime includes both start and end times, where the parent event shows the start of the first transect and the end of the last transect, whereas the child event shows the start and end time of that individual 50 m transect walk. For this data set, the event time varies according to how many insects that were registered (and potentially handled for identificaton), but the walking speed when not handling a specimen is fixed, ensuring a fixed sampling effort.\r\n\r\neventDate\r\neventTime\r\neventRemarks\r\n2013-05-18\r\n2013-05-18 11:06:00/2013-05-18 11:46:00\r\nFixed 1.5x1.5km square with 20 fixed transect walks of 50m each.\r\n2013-05-18\r\n2013-05-18 11:06:00/2013-05-18 11:08:00\r\nFixed transect walk of 50m.\r\n\r\nThe next columns include data on the survey location, where the parent id has coordinates for the whole polygon that make up the survey square, and the child event has coordinates for the linestring that constitutes the transect walk. We won’t show all columns here.\r\n\r\nlocationID\r\nfootprintWKT\r\n4d09c65d-8487-43b0-8852-077034be6e75\r\nPOLYGON((6.7409163667009 58.3543225039069,6.73778059597473 58.3676549688298,6.76315583047638 58.369300865387,6.76628223764961 58.3559675476723,6.7409163667009 58.3543225039069))\r\n56835e4d-f268-42d6-86cc-610ffd62550b\r\nLINESTRING(6.764957 58.357024,6.765447 58.356613)\r\n\r\nThe occurrence table\r\nNow for the actual occurrence data, shown again as the two first rows of a tibble.\r\n\r\n\r\n\r\n\r\n\r\n# A tibble: 1 x 19\r\n  id    modified            basisOfRecord occurrenceID individualCount\r\n  <chr> <dttm>              <chr>         <chr>                  <dbl>\r\n1 6aab… 2020-04-20 11:53:51 HumanObserva… 501209e5-6c…               0\r\n# … with 14 more variables: sex <lgl>, lifeStage <chr>,\r\n#   occurrenceStatus <chr>, eventID <chr>, taxonID <chr>,\r\n#   scientificName <chr>, kingdom <chr>, phylum <chr>, class <chr>,\r\n#   order <chr>, family <chr>, genus <chr>, specificEpithet <chr>,\r\n#   vernacularName <chr>\r\n\r\nThe first 4 columns include a record id, a last modified record, basis of record and a unique occurrence ID.\r\n\r\nid\r\nmodified\r\nbasisOfRecord\r\noccurrenceID\r\n6aab1176-572b-4e13-b7d1-6d2a5a217961\r\n2020-04-20 11:53:51\r\nHumanObservation\r\n501209e5-6cc3-4538-84b8-d1e5d0cdb279\r\n\r\n\r\n\r\n\r\nFigure 4: Bombus hortorum on a Trifolium repens.\r\n\r\n\r\n\r\nThe next 5 include information of the individual count, sex of the individual (which we don’t record), life stage, occurrence status (present/absent), and the eventID, which links the data back to the event table.\r\n\r\nindividualCount\r\nsex\r\nlifeStage\r\noccurrenceStatus\r\neventID\r\n0\r\nNA\r\nAdult\r\nabsent\r\n6aab1176-572b-4e13-b7d1-6d2a5a217961\r\n\r\nThe next colums include taxonomic information, with the possibility to include a URL for the species and a local vernacular name. Not all columns are shown. If you are interested, pictures of the species is available at the URL by clicking “Les mer om taksonet”.\r\n\r\ntaxonID\r\nscientificName\r\nkingdom\r\nphylum\r\nclass\r\nvernacularName\r\nhttps://www.biodiversity.no/ScientificName/46614\r\nBoloria aquilonaris\r\nAnimalia\r\nArthropoda\r\nInsecta\r\nMyrperlemorvinge\r\n\r\n\r\n\r\n\r\nFigure 5: Bombus consobrinus approaching an Aconitum lycoctonum.\r\n\r\n\r\n\r\nUploading and downloading data\r\nSo that’s it in a nutshell for the formatting of the data. We have managed to encode all our recorded data in a quite complex survey scheme, including zero counts, into GBIF. The next step is to provide the relevant metadata, and upload the data to the GBIF. Note that GBIF only accepts data through their partner organisations, so you will have to find your nearest GBIF affiliate and send it through there. These organizations typically push data to GBIF through a specific tool called an IPT. Our organisation for example hosts its own IPT. For more information on publishing, see https://www.gbif.org/publishing-data, where you can also find excel templates for event core data.\r\nThe data set that were featured here is available at https://www.gbif.org/dataset/aea17af8-5578-4b04-b5d3-7adf0c5a1e60.\r\nAnd back again?\r\nOnce the data is stored on GBIF, it can be downloaded through their standard tools. Recreating the original structure of course also requires some work, and it is adviced that you publish a recipe specific to your data, that you can refer users to. Such a recipe for our example data is available at https://github.com/jenast/NBBM_data_export/blob/master/NBBM_GBIF_to_BMS_export.md\r\nA last word of advice\r\nAt the moment, there doesn’t exist much prepackaged tools that help with the formatting of these type of data, so it will most likely be a custom job for your case. Of course, if you plan on submitting your data to GBIF in this way, the earlier you set up your data structure to harmonize with GBIF, the easier it will be. That could be e.g. storing event ID’s and parent event ID’s along with your readable survey and location codes. At some point, you will probably have to merge an event table together with one or more parent event tables, as you are unlikely to store this together from the outset. The key thing is to keep track of the event and parent event ID’s so that they codify the dependence structure of the data set. Hopefully there will be some custom tools for this type of work developed in the future, for example as an R-package.\r\nFor our data set, we already had the data in a PostgreSQL database. After inserting some additional data that GBIF required and sorting out some relationships in the database, it was a matter of setting up views that formatted the data into the GBIF event core format. Once that was done, we set up permissions so that the IPT machine could access these export views directly, to streamline the publishing pipeline. Future additions to the data should then go smoothly.\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2020-08-12T10:33:27+02:00",
    "input_file": {}
  },
  {
    "path": "posts/",
    "title": "How to initiate and organise a data project in ecology?",
    "description": {},
    "author": [
      {
        "name": "Matt & Erlend",
        "url": {}
      }
    ],
    "date": "2020-06-03",
    "categories": [],
    "contents": "\r\nLiving Norway\r\nThe Living Norway Ecological Data Network has the ambition to become the focal point for ecological data in Norway. The primary objective of Living Norway will be to provide infrastructure for cutting-edge and transparent science of the highest societal relevance in ecology, including the associated fields of biodiversity, conservation biology, taxonomy, systematics and evolution.\r\nLivingNorwayR\r\nThe R package associated with Living Norway, “LivingNorwayR” is currently in development, here we demonstrate the use of the first function which allows the user to set up a logical folder structure in which to store all files associated with the data collected. This folder structure serves as the architecture for a “data package” which includes not only raw data, but digitised field notes, data that has been manipulated for analysis, analysis scripts, etc. In addition, data documentation such as data management plans and metadata should be included in the “data package”.\r\nNot all projects will rely on the same underlying data flow model, but in our experience most field based ecology projects still have sufficient overlap in terms of data flow to make it worthwhile suggesting a common folder structure for field data projects. Beyond making it easier for the individual researchers or data management units to keep their data well organised, an important endpoint is to facilitate publication of the “data package” at the appropriate stage in the project lifecycle. Thus, the folder structure needs to facilitate publishing of the mapped data as a Darwin Core Archive (and preferentially register the data set with GBIF). Also the raw data could be easily extracted and archived in a generalist repository.\r\nCreating your data package structure\r\nWe recommend working within an RStudio project so that you have more control about where your “working environment” is situated (this is folder where, on your computer, R saves files to). Once you have set-up a project (open RStudio select “File” then “New Project”), you can install and load LivingNorwayR.\r\n\r\n\r\n#Install and load packages\r\n# If you need to install packages first remove the \"#\" sign (which in R language means that the line is not run as code)\r\n#install.packages(devtools)\r\nlibrary(devtools)\r\n#devtools::install_github(\"LivingNorway/LivingNorwayR\")\r\nlibrary(LivingNorwayR)\r\n\r\nCreate the folder structure\r\nFirst we need to give the project a name. R does not like names to be too complex and to contain special characters or spaces. Please use \"_\" instead of a space in the project name (and in all your R work - it will make your life a lot easier).\r\n\r\n\r\nproject_name=\"My_First_LivingNorway_prj\"\r\nbuild_folder_structure(project_name = project_name)\r\n\r\nOnce you run the build_folder_structure() function you end up with a folder containing several sub-folders in your project. Let’s explore these folders.\r\n\r\n\r\nlibrary(plyr)\r\npath<-paste0(getwd(),\"/\", project_name)\r\ndirslist<-list.dirs(path,recursive = TRUE)\r\nx <- lapply(strsplit(dirslist, \"/\"), function(z) as.data.frame(t(z)))\r\nLivingNorway_project <- rbind.fill(x)\r\nlibrary(collapsibleTree) \r\np <- collapsibleTree( LivingNorway_project, c(\"V8\", \"V9\"))\r\np\r\n\r\n{\"x\":{\"data\":{\"name\":\"LivingNorway_project\",\"children\":[{\"name\":\"My_First_LivingNorway_prj\",\"children\":[{\"name\":\"data\"},{\"name\":\"dmp\"},{\"name\":\"meta_xml\"},{\"name\":\"minimum_metadata\"},{\"name\":\"scripts\"}]}]},\"options\":{\"hierarchy\":[\"V8\",\"V9\"],\"input\":null,\"attribute\":\"leafCount\",\"linkLength\":null,\"fontSize\":10,\"tooltip\":false,\"collapsed\":true,\"zoomable\":true,\"margin\":{\"top\":20,\"bottom\":20,\"left\":125,\"right\":105},\"fill\":\"lightsteelblue\"}},\"evals\":[],\"jsHooks\":[]}\r\nFrom the tree diagram you can explore the folder structure (click on the blue nodes to expand them). At the moment we have 5 folders in the top-most heirachy: data, dmp,meta_xml,minimum_metadata and scripts.\r\ndata\r\nThe data folder contains three other folders: mapped_data, raw_data and scan_data. Let’s start with the raw_data folder. In this folder you can place the data in its rawest form (i.e. no manipulation or data cleaning has taken place). Raw_data that you subsequently clean or transform in anyway needs to be stored in the mapped_data folder. It is really useful to be able to go back to the original data especially if something happens to the datafiles that you are working on for the analysis (the mapped_data file(s)). The scan_data folder contains scanned copies of non-digital raw data (field collection notes for example).\r\ndmp\r\nThe dmp folder contains your data management plan. A data management plan is “a written document that describes the data you expect to acquire or generate during the course of a research project, how you will manage, describe, analyze, and store those data, and what mechanisms you will use at the end of your project to share and preserve your data” (https://library.stanford.edu/research/data-management-services/data-management-plans).\r\nmeta_xml\r\nThis folder contains metadata in a machine readable form using the EML metadata language.\r\nscripts\r\nAny code (e.g. python or R) used to manipulate the raw data, download publically available covariates, etc. should be saved in this folder. All scripts should be reproducilble by others not using your specifc computer (so not calling on locally stored files or functions).\r\nMetadata\r\nMetadata (a set of variables that describes the raw or transformed dataset) are required so that future users (including your future self) get an understanding of, for example, where the data was collected, who collected it and what units the variables were measured in. Basic mimimum metadata should be added as soon as possible (before you forget it).\r\nNow your folder structure is ready. The next step is to load your files in to the correct folders\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2020-06-03T17:17:16+02:00",
    "input_file": {}
  }
]
